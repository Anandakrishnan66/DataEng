{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90f92f6f-1ed1-4d81-835c-444518188f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext \n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76031b0-d253-4505-a827-8639373c7e75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3197823526264055#setting/sparkui/0724-044622-t9g6hw7b/driver-8268429045387300894\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.4:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.139.64.4:7077 appName=Databricks Shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806898ff-e4aa-40c8-b481-c704262b0571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3197823526264055#setting/sparkui/0724-044622-t9g6hw7b/driver-8268429045387300894\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.4:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd73eeee90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db45979-997d-4635-a2a0-31d81169cbc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "greentaxi_df=spark.read.csv(\"/Volumes/azuredatabricks_2000/default/greentaxi/2021_Green_Taxi_Trip_Data.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a51f875-cd47-4f9b-bf38-3bdf24daf62c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "greentaxi_df_final=greentaxi_df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7e4b15-b60d-4699-ac30-1a3e358ec578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# greentaxi_df_final.write.mode(\"overwrite\").option(\"header\",\"true\").parquet(\"/Volumes/azuredatabricks_2000/default/greentaxi/greenparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095de818-ae05-4a12-88b9-88157a21c19a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema=greentaxi_df_final.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94b7b0c-54c4-4d91-8ef5-d798a3b91aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxiz_df=spark.readStream.schema(schema=schema).format(\"csv\").option(\"header\",\"true\").load(\"/Volumes/azuredatabricks_2000/default/greentaxi/*parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c406f03-1525-428d-8315-fe4fe6780b64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "\n",
    "tx_transformation = taxiz_df\\\n",
    "    .withColumn('timestamp', current_timestamp())\\\n",
    "    .groupBy(\"payment_type\").sum(\"fare_amount\").alias(\"fare_amounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059b07b2-a31d-4faa-8de6-cc1d886854d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "df2 = df_stream \\\n",
    "              .withColumn('timestamp', unix_timestamp(col('EventDate'), \"MM/dd/yyyy hh:mm:ss aa\").cast(TimestampType())) \\\n",
    "              .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "              .groupBy(col(\"SendID\"), \"timestamp\") \\\n",
    "              .agg(max(col('timestamp')).alias(\"timestamp\")) \\\n",
    "              .orderBy('timestamp', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a93f0f-7a7d-4f92-851a-9739b9529f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greentaxi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c7ff9e-22b6-4905-9ccb-8556d9ef813c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[payment_type: int, sum(fare_amount): double, timestamp: timestamp]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6a3929-bb33-4be5-941a-79cd7a1fcc81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1194792206002261>, line 24\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m aggregated_df \u001B[38;5;241m=\u001B[39m tx_transformation_with_watermark \\\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\n",
       "\u001B[1;32m     10\u001B[0m         window(tx_transformation_with_watermark\u001B[38;5;241m.\u001B[39mtimestamp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5 minutes\u001B[39m\u001B[38;5;124m\"\u001B[39m),  \n",
       "\u001B[1;32m     11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpayment_type\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     12\u001B[0m     ) \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfare_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     16\u001B[0m tx_stream \u001B[38;5;241m=\u001B[39m aggregated_df \\\n",
       "\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/azuredatabricks_2000/default/checkpoint/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     22\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[0;32m---> 24\u001B[0m tx_stream\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n",
       "\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\n",
       "Number of partitions from OffsetLog: 200 \n",
       "Offending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\n",
       "Paths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST\n",
       "=== Streaming Query ===\n",
       "Identifier: [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091]\n",
       "Current Start Offsets: {}\n",
       "Current End Offsets: {}\n",
       "\n",
       "Current State: ACTIVE\n",
       "Thread State: RUNNABLE\n",
       "\n",
       "Logical Plan:\n",
       "WriteToMicroBatchDataSourceV1 FileSink[/Volumes/azuredatabricks_2000/default/writestream], 468e1175-d884-4d67-8626-ab3235d22bcb, [path=/Volumes/azuredatabricks_2000/default/writestream, checkpointLocation=/Volumes/azuredatabricks_2000/default/checkpoint/], Append\n",
       "+- Aggregate [window#3787-T600000ms, payment_type#3718], [window#3787-T600000ms, payment_type#3718, sum(fare_amount#3710) AS sum(fare_amount)#3786]\n",
       "   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#3787-T600000ms, VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, timestamp#3741-T600000ms]\n",
       "      +- Filter isnotnull(timestamp#3741-T600000ms)\n",
       "         +- EventTimeWatermark timestamp#3741: timestamp, 10 minutes\n",
       "            +- Project [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, current_timestamp() AS timestamp#3741]\n",
       "               +- StreamingExecutionRelation FileStreamSource[dbfs:/Volumes/azuredatabricks_2000/default/greentaxi/*parquet], [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "StreamingQueryException",
        "evalue": "[STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\nNumber of partitions from OffsetLog: 200 \nOffending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\nPaths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST\n=== Streaming Query ===\nIdentifier: [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091]\nCurrent Start Offsets: {}\nCurrent End Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSourceV1 FileSink[/Volumes/azuredatabricks_2000/default/writestream], 468e1175-d884-4d67-8626-ab3235d22bcb, [path=/Volumes/azuredatabricks_2000/default/writestream, checkpointLocation=/Volumes/azuredatabricks_2000/default/checkpoint/], Append\n+- Aggregate [window#3787-T600000ms, payment_type#3718], [window#3787-T600000ms, payment_type#3718, sum(fare_amount#3710) AS sum(fare_amount)#3786]\n   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#3787-T600000ms, VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, timestamp#3741-T600000ms]\n      +- Filter isnotnull(timestamp#3741-T600000ms)\n         +- EventTimeWatermark timestamp#3741: timestamp, 10 minutes\n            +- Project [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, current_timestamp() AS timestamp#3741]\n               +- StreamingExecutionRelation FileStreamSource[dbfs:/Volumes/azuredatabricks_2000/default/greentaxi/*parquet], [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720]\n"
       },
       "metadata": {
        "errorSummary": "[STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\nNumber of partitions from OffsetLog: 200 \nOffending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\nPaths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "STREAM_FAILED",
        "sqlState": "XXKST",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)",
        "File \u001B[0;32m<command-1194792206002261>, line 24\u001B[0m\n\u001B[1;32m      8\u001B[0m aggregated_df \u001B[38;5;241m=\u001B[39m tx_transformation_with_watermark \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\n\u001B[1;32m     10\u001B[0m         window(tx_transformation_with_watermark\u001B[38;5;241m.\u001B[39mtimestamp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5 minutes\u001B[39m\u001B[38;5;124m\"\u001B[39m),  \n\u001B[1;32m     11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpayment_type\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     12\u001B[0m     ) \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfare_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m tx_stream \u001B[38;5;241m=\u001B[39m aggregated_df \\\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/azuredatabricks_2000/default/checkpoint/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m---> 24\u001B[0m tx_stream\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\nNumber of partitions from OffsetLog: 200 \nOffending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\nPaths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST\n=== Streaming Query ===\nIdentifier: [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091]\nCurrent Start Offsets: {}\nCurrent End Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSourceV1 FileSink[/Volumes/azuredatabricks_2000/default/writestream], 468e1175-d884-4d67-8626-ab3235d22bcb, [path=/Volumes/azuredatabricks_2000/default/writestream, checkpointLocation=/Volumes/azuredatabricks_2000/default/checkpoint/], Append\n+- Aggregate [window#3787-T600000ms, payment_type#3718], [window#3787-T600000ms, payment_type#3718, sum(fare_amount#3710) AS sum(fare_amount)#3786]\n   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#3787-T600000ms, VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, timestamp#3741-T600000ms]\n      +- Filter isnotnull(timestamp#3741-T600000ms)\n         +- EventTimeWatermark timestamp#3741: timestamp, 10 minutes\n            +- Project [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, current_timestamp() AS timestamp#3741]\n               +- StreamingExecutionRelation FileStreamSource[dbfs:/Volumes/azuredatabricks_2000/default/greentaxi/*parquet], [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720]\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, window\n",
    "\n",
    "\n",
    "taxiz_df_final = taxiz_df.withColumn(\"timestamp\", current_timestamp())\n",
    "tx_transformation_with_watermark = taxiz_df_final \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")  \n",
    "    \n",
    "aggregated_df = tx_transformation_with_watermark \\\n",
    "    .groupBy(\n",
    "        window(tx_transformation_with_watermark.timestamp, \"5 minutes\"),  \n",
    "        \"payment_type\"\n",
    "    ) \\\n",
    "    .sum(\"fare_amount\")\n",
    "\n",
    "\n",
    "tx_stream = aggregated_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/Volumes/azuredatabricks_2000/default/writestream\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/azuredatabricks_2000/default/checkpoint/\") \\\n",
    "    .start()\n",
    "\n",
    "tx_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545fa5f1-4407-40c3-9027-152a68bb051d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1194792206002259>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mtaxiz_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3732\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   3699\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   3700\u001B[0m \n",
       "\u001B[1;32m   3701\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3729\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   3730\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 3732\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n",
       "\u001B[1;32m   3733\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n",
       "\u001B[1;32m   3734\u001B[0m     )\n",
       "\u001B[1;32m   3735\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkAttributeError",
        "evalue": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
       },
       "metadata": {
        "errorSummary": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "ATTRIBUTE_NOT_SUPPORTED",
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-1194792206002259>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtaxiz_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3732\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   3699\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   3700\u001B[0m \n\u001B[1;32m   3701\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3729\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   3730\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 3732\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n\u001B[1;32m   3733\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n\u001B[1;32m   3734\u001B[0m     )\n\u001B[1;32m   3735\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
        "\u001B[0;31mPySparkAttributeError\u001B[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxiz_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3799bc51-2ba8-4080-8b2c-10320f2519ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataBricks-Realtime StreamingAnalysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
