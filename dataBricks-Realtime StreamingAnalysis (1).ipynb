{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90f92f6f-1ed1-4d81-835c-444518188f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext \n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76031b0-d253-4505-a827-8639373c7e75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3197823526264055#setting/sparkui/0724-044622-t9g6hw7b/driver-8268429045387300894\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.4:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://10.139.64.4:7077 appName=Databricks Shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806898ff-e4aa-40c8-b481-c704262b0571",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3197823526264055#setting/sparkui/0724-044622-t9g6hw7b/driver-8268429045387300894\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.4:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd73eeee90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db45979-997d-4635-a2a0-31d81169cbc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "greentaxi_df=spark.read.csv(\"/Volumes/azuredatabricks_2000/default/greentaxi/2021_Green_Taxi_Trip_Data.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a51f875-cd47-4f9b-bf38-3bdf24daf62c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "greentaxi_df_final=greentaxi_df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7e4b15-b60d-4699-ac30-1a3e358ec578",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "greentaxi_df_final.write.mode(\"overwrite\").option(\"header\",\"true\").parquet(\"/Volumes/azuredatabricks_2000/default/taxi_out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095de818-ae05-4a12-88b9-88157a21c19a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema=greentaxi_df_final.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94b7b0c-54c4-4d91-8ef5-d798a3b91aa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "taxiz_df=spark.readStream.schema(schema=schema).format(\"csv\").option(\"header\",\"true\").load(\"/Volumes/azuredatabricks_2000/default/taxi_out/*parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c406f03-1525-428d-8315-fe4fe6780b64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "\n",
    "tx_transformation = taxiz_df\\\n",
    "    .withColumn('timestamp', current_timestamp())\\\n",
    "    .groupBy(\"payment_type\").sum(\"fare_amount\").alias(\"fare_amounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "059b07b2-a31d-4faa-8de6-cc1d886854d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "df2 = df_stream \\\n",
    "              .withColumn('timestamp', unix_timestamp(col('EventDate'), \"MM/dd/yyyy hh:mm:ss aa\").cast(TimestampType())) \\\n",
    "              .withWatermark(\"timestamp\", \"1 minutes\") \\\n",
    "              .groupBy(col(\"SendID\"), \"timestamp\") \\\n",
    "              .agg(max(col('timestamp')).alias(\"timestamp\")) \\\n",
    "              .orderBy('timestamp', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5a93f0f-7a7d-4f92-851a-9739b9529f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greentaxi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c7ff9e-22b6-4905-9ccb-8556d9ef813c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[payment_type: int, sum(fare_amount): double, timestamp: timestamp]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6a3929-bb33-4be5-941a-79cd7a1fcc81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1194792206002261>, line 24\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m aggregated_df \u001B[38;5;241m=\u001B[39m tx_transformation_with_watermark \\\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\n",
       "\u001B[1;32m     10\u001B[0m         window(tx_transformation_with_watermark\u001B[38;5;241m.\u001B[39mtimestamp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5 minutes\u001B[39m\u001B[38;5;124m\"\u001B[39m),  \n",
       "\u001B[1;32m     11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpayment_type\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     12\u001B[0m     ) \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfare_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     16\u001B[0m tx_stream \u001B[38;5;241m=\u001B[39m aggregated_df \\\n",
       "\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/azuredatabricks_2000/default/checkpoint/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     22\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[0;32m---> 24\u001B[0m tx_stream\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n",
       "\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\n",
       "Number of partitions from OffsetLog: 200 \n",
       "Offending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\n",
       "Paths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST\n",
       "=== Streaming Query ===\n",
       "Identifier: [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091]\n",
       "Current Start Offsets: {}\n",
       "Current End Offsets: {}\n",
       "\n",
       "Current State: ACTIVE\n",
       "Thread State: RUNNABLE\n",
       "\n",
       "Logical Plan:\n",
       "WriteToMicroBatchDataSourceV1 FileSink[/Volumes/azuredatabricks_2000/default/writestream], 468e1175-d884-4d67-8626-ab3235d22bcb, [path=/Volumes/azuredatabricks_2000/default/writestream, checkpointLocation=/Volumes/azuredatabricks_2000/default/checkpoint/], Append\n",
       "+- Aggregate [window#3787-T600000ms, payment_type#3718], [window#3787-T600000ms, payment_type#3718, sum(fare_amount#3710) AS sum(fare_amount)#3786]\n",
       "   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#3787-T600000ms, VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, timestamp#3741-T600000ms]\n",
       "      +- Filter isnotnull(timestamp#3741-T600000ms)\n",
       "         +- EventTimeWatermark timestamp#3741: timestamp, 10 minutes\n",
       "            +- Project [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, current_timestamp() AS timestamp#3741]\n",
       "               +- StreamingExecutionRelation FileStreamSource[dbfs:/Volumes/azuredatabricks_2000/default/greentaxi/*parquet], [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "StreamingQueryException",
        "evalue": "[STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\nNumber of partitions from OffsetLog: 200 \nOffending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\nPaths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST\n=== Streaming Query ===\nIdentifier: [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091]\nCurrent Start Offsets: {}\nCurrent End Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSourceV1 FileSink[/Volumes/azuredatabricks_2000/default/writestream], 468e1175-d884-4d67-8626-ab3235d22bcb, [path=/Volumes/azuredatabricks_2000/default/writestream, checkpointLocation=/Volumes/azuredatabricks_2000/default/checkpoint/], Append\n+- Aggregate [window#3787-T600000ms, payment_type#3718], [window#3787-T600000ms, payment_type#3718, sum(fare_amount#3710) AS sum(fare_amount)#3786]\n   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#3787-T600000ms, VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, timestamp#3741-T600000ms]\n      +- Filter isnotnull(timestamp#3741-T600000ms)\n         +- EventTimeWatermark timestamp#3741: timestamp, 10 minutes\n            +- Project [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, current_timestamp() AS timestamp#3741]\n               +- StreamingExecutionRelation FileStreamSource[dbfs:/Volumes/azuredatabricks_2000/default/greentaxi/*parquet], [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720]\n"
       },
       "metadata": {
        "errorSummary": "[STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\nNumber of partitions from OffsetLog: 200 \nOffending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\nPaths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "STREAM_FAILED",
        "sqlState": "XXKST",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)",
        "File \u001B[0;32m<command-1194792206002261>, line 24\u001B[0m\n\u001B[1;32m      8\u001B[0m aggregated_df \u001B[38;5;241m=\u001B[39m tx_transformation_with_watermark \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39mgroupBy(\n\u001B[1;32m     10\u001B[0m         window(tx_transformation_with_watermark\u001B[38;5;241m.\u001B[39mtimestamp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5 minutes\u001B[39m\u001B[38;5;124m\"\u001B[39m),  \n\u001B[1;32m     11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpayment_type\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     12\u001B[0m     ) \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39msum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfare_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m tx_stream \u001B[38;5;241m=\u001B[39m aggregated_df \\\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Volumes/azuredatabricks_2000/default/checkpoint/\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m---> 24\u001B[0m tx_stream\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:221\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091] terminated with exception: All stateful operators must have the same number of partitions as in the offsetLog.\nNumber of partitions from OffsetLog: 200 \nOffending state op ids and actual number partitions as tuple (state op, num partition): (1,6)\nPaths of offending states: dbfs:/Volumes/azuredatabricks_2000/default/checkpoint/state/1 SQLSTATE: XXKST\n=== Streaming Query ===\nIdentifier: [id = 468e1175-d884-4d67-8626-ab3235d22bcb, runId = 594e1463-40d8-4842-a4fc-5f81bd180091]\nCurrent Start Offsets: {}\nCurrent End Offsets: {}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nWriteToMicroBatchDataSourceV1 FileSink[/Volumes/azuredatabricks_2000/default/writestream], 468e1175-d884-4d67-8626-ab3235d22bcb, [path=/Volumes/azuredatabricks_2000/default/writestream, checkpointLocation=/Volumes/azuredatabricks_2000/default/checkpoint/], Append\n+- Aggregate [window#3787-T600000ms, payment_type#3718], [window#3787-T600000ms, payment_type#3718, sum(fare_amount#3710) AS sum(fare_amount)#3786]\n   +- Project [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) + 300000000) ELSE ((precisetimestampconversion(timestamp#3741-T600000ms, TimestampType, LongType) - 0) % 300000000) END) - 0) + 300000000), LongType, TimestampType))) AS window#3787-T600000ms, VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, timestamp#3741-T600000ms]\n      +- Filter isnotnull(timestamp#3741-T600000ms)\n         +- EventTimeWatermark timestamp#3741: timestamp, 10 minutes\n            +- Project [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720, current_timestamp() AS timestamp#3741]\n               +- StreamingExecutionRelation FileStreamSource[dbfs:/Volumes/azuredatabricks_2000/default/greentaxi/*parquet], [VendorID#3701, lpep_pickup_datetime#3702, lpep_dropoff_datetime#3703, store_and_fwd_flag#3704, RatecodeID#3705, PULocationID#3706, DOLocationID#3707, passenger_count#3708, trip_distance#3709, fare_amount#3710, extra#3711, mta_tax#3712, tip_amount#3713, tolls_amount#3714, ehail_fee#3715, improvement_surcharge#3716, total_amount#3717, payment_type#3718, trip_type#3719, congestion_surcharge#3720]\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, window\n",
    "\n",
    "\n",
    "taxiz_df_final = taxiz_df.withColumn(\"timestamp\", current_timestamp())\n",
    "tx_transformation_with_watermark = taxiz_df_final \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")  \n",
    "    \n",
    "aggregated_df = tx_transformation_with_watermark \\\n",
    "    .groupBy(\n",
    "        window(tx_transformation_with_watermark.timestamp, \"5 minutes\"),  \n",
    "        \"payment_type\"\n",
    "    ) \\\n",
    "    .sum(\"fare_amount\")\n",
    "\n",
    "\n",
    "tx_stream = aggregated_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/Volumes/azuredatabricks_2000/default/writestream\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/azuredatabricks_2000/default/checkpoint/\") \\\n",
    "    .start()\n",
    "\n",
    "tx_stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545fa5f1-4407-40c3-9027-152a68bb051d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1194792206002259>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mtaxiz_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3732\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   3699\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   3700\u001B[0m \n",
       "\u001B[1;32m   3701\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3729\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   3730\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   3731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 3732\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n",
       "\u001B[1;32m   3733\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n",
       "\u001B[1;32m   3734\u001B[0m     )\n",
       "\u001B[1;32m   3735\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mPySparkAttributeError\u001B[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkAttributeError",
        "evalue": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
       },
       "metadata": {
        "errorSummary": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "ATTRIBUTE_NOT_SUPPORTED",
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkAttributeError\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-1194792206002259>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtaxiz_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3732\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   3699\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   3700\u001B[0m \n\u001B[1;32m   3701\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3729\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   3730\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3731\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 3732\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkAttributeError(\n\u001B[1;32m   3733\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mATTRIBUTE_NOT_SUPPORTED\u001B[39m\u001B[38;5;124m\"\u001B[39m, message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattr_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: name}\n\u001B[1;32m   3734\u001B[0m     )\n\u001B[1;32m   3735\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
        "\u001B[0;31mPySparkAttributeError\u001B[0m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `stop` is not supported."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxiz_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3799bc51-2ba8-4080-8b2c-10320f2519ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab49f34e-e943-46e6-a3d1-c9f74918bc8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, lpep_pickup_datetime: string, lpep_dropoff_datetime: string, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: string, improvement_surcharge: double, total_amount: double, payment_type: int, trip_type: int, congestion_surcharge: double, pickuptime: timestamp]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxiz_df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767672b9-5f10-4d23-9cc4-f241b3289d87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:718)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:437)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:437)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1266)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:983)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:944)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:718)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:437)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:437)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1266)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:983)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:944)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:782)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:808)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:807)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:862)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:655)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Assuming taxiz_df is already defined and contains the column 'lpep_pickup_datetime'\n",
    "taxiz_df = taxiz_df.withColumn(\"pickuptime\", to_timestamp(col(\"lpep_pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Use the 'pickuptime' column for watermarking and grouping\n",
    "taxitransformation = taxiz_df.withWatermark(\"pickuptime\", \"10 minutes\") \\\n",
    "    .groupBy('trip_type', 'pickuptime').sum('fare_amount')\n",
    "\n",
    "# Correct the format option and add the missing leading slash to the checkpoint location\n",
    "writetsream = taxitransformation.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/Volumes/azuredatabricks_2000/default/writestream/\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/azuredatabricks_2000/default/checkpoint2/\") \\\n",
    "    .start()\n",
    "\n",
    "writetsream.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d264460-85b6-4ede-a0c9-09edf1db81c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_df= spark.read.parquet(\"/Volumes/azuredatabricks_2000/default/taxi_out/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfa1569-f205-4ca2-8cef-ab635974335a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|       2|04/06/2021 05:15:...| 04/06/2021 05:41:...|                 N|         1|          43|          90|              1|         4.32|       18.5|  1.0|    0.5|       1.0|         0.0|     NULL|                  0.3|       24.05|           1|        1|                2.75|\n|       1|01/27/2021 12:38:...| 01/27/2021 12:57:...|                 N|         1|          25|         227|              1|          4.0|       16.0|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        16.8|           2|        1|                 0.0|\n|    NULL|02/21/2021 01:01:...| 02/21/2021 01:16:...|              NULL|      NULL|         243|         116|           NULL|         2.41|      10.95|  0.0|    0.0|      2.47|         0.0|     NULL|                  0.3|       13.72|        NULL|     NULL|                NULL|\n|       2|03/24/2021 06:45:...| 03/24/2021 07:32:...|                 N|         1|          97|         163|              1|         8.55|       34.5|  1.0|    0.5|      7.81|         0.0|     NULL|                  0.3|       46.86|           1|        1|                2.75|\n|    NULL|01/20/2021 03:56:...| 01/20/2021 04:19:...|              NULL|      NULL|         117|         155|           NULL|         8.39|      33.77| 2.75|    0.0|       0.0|        2.29|     NULL|                  0.3|       39.11|        NULL|     NULL|                NULL|\n|    NULL|03/20/2021 01:03:...| 03/20/2021 01:19:...|              NULL|      NULL|          63|          39|           NULL|         3.14|      12.68|  5.5|    0.0|       0.0|         0.0|     NULL|                  0.3|       18.48|        NULL|     NULL|                NULL|\n|    NULL|03/14/2021 01:26:...| 03/14/2021 01:50:...|              NULL|      NULL|         254|         153|           NULL|         6.99|      27.43| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       30.48|        NULL|     NULL|                NULL|\n|    NULL|02/05/2021 10:47:...| 02/05/2021 10:54:...|              NULL|      NULL|          76|          63|           NULL|         1.04|      15.43| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       18.48|        NULL|     NULL|                NULL|\n|       2|04/28/2021 04:21:...| 04/28/2021 05:09:...|                 N|         1|          33|          29|              1|        13.17|       43.5|  1.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        45.3|           1|        1|                 0.0|\n|    NULL|01/06/2021 04:45:...| 01/06/2021 05:29:...|              NULL|      NULL|          89|           1|           NULL|          0.0|       62.2|  1.0|    0.5|     10.58|       19.87|     NULL|                  0.3|       96.45|        NULL|     NULL|                NULL|\n|       2|01/09/2021 05:47:...| 01/09/2021 06:06:...|                 N|         1|          82|          56|              1|         2.01|       12.5|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        13.3|           2|        1|                 0.0|\n|    NULL|03/24/2021 12:46:...| 03/24/2021 01:05:...|              NULL|      NULL|          76|         122|           NULL|        10.07|      33.06| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       36.11|        NULL|     NULL|                NULL|\n|       2|02/25/2021 07:50:...| 02/25/2021 08:04:...|                 N|         1|          74|          43|              1|         1.37|        9.5|  0.0|    0.5|      2.06|         0.0|     NULL|                  0.3|       12.36|           1|        1|                 0.0|\n|       2|04/15/2021 05:44:...| 04/15/2021 06:10:...|                 N|         1|          17|          72|              1|         4.01|       18.5|  1.0|    0.5|      4.06|         0.0|     NULL|                  0.3|       24.36|           1|        1|                 0.0|\n|       2|02/24/2021 05:36:...| 02/24/2021 05:45:...|                 N|         1|          43|          75|              1|         1.13|        7.5|  1.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         9.3|           2|        1|                 0.0|\n|       2|04/25/2021 12:49:...| 04/25/2021 12:51:...|                 N|         1|          41|          41|              1|         0.58|        4.0|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         4.8|           2|        1|                 0.0|\n|       2|03/02/2021 05:59:...| 03/02/2021 06:43:...|                 N|         1|         197|         130|              1|         6.53|       30.0|  1.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        31.8|           1|        1|                 0.0|\n|       1|02/06/2021 02:29:...| 02/06/2021 02:41:...|                 N|         1|          74|         179|              1|          3.7|       13.0|  0.0|    0.5|      3.95|        6.12|     NULL|                  0.3|       23.87|           1|        1|                 0.0|\n|    NULL|03/27/2021 08:27:...| 03/27/2021 08:45:...|              NULL|      NULL|          97|         133|           NULL|         7.72|      24.63| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       27.68|        NULL|     NULL|                NULL|\n|    NULL|02/17/2021 07:51:...| 02/17/2021 08:01:...|              NULL|      NULL|          61|          61|           NULL|         0.67|      11.95| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|        15.0|        NULL|     NULL|                NULL|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "read_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3a9174-2ac3-4b65-9084-32ac5b1d71d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|       2|04/06/2021 05:15:...| 04/06/2021 05:41:...|                 N|         1|          43|          90|              1|         4.32|       18.5|  1.0|    0.5|       1.0|         0.0|     NULL|                  0.3|       24.05|           1|        1|                2.75|\n|       1|01/27/2021 12:38:...| 01/27/2021 12:57:...|                 N|         1|          25|         227|              1|          4.0|       16.0|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        16.8|           2|        1|                 0.0|\n|    NULL|02/21/2021 01:01:...| 02/21/2021 01:16:...|              NULL|      NULL|         243|         116|           NULL|         2.41|      10.95|  0.0|    0.0|      2.47|         0.0|     NULL|                  0.3|       13.72|        NULL|     NULL|                NULL|\n|       2|03/24/2021 06:45:...| 03/24/2021 07:32:...|                 N|         1|          97|         163|              1|         8.55|       34.5|  1.0|    0.5|      7.81|         0.0|     NULL|                  0.3|       46.86|           1|        1|                2.75|\n|    NULL|01/20/2021 03:56:...| 01/20/2021 04:19:...|              NULL|      NULL|         117|         155|           NULL|         8.39|      33.77| 2.75|    0.0|       0.0|        2.29|     NULL|                  0.3|       39.11|        NULL|     NULL|                NULL|\n|    NULL|03/20/2021 01:03:...| 03/20/2021 01:19:...|              NULL|      NULL|          63|          39|           NULL|         3.14|      12.68|  5.5|    0.0|       0.0|         0.0|     NULL|                  0.3|       18.48|        NULL|     NULL|                NULL|\n|    NULL|03/14/2021 01:26:...| 03/14/2021 01:50:...|              NULL|      NULL|         254|         153|           NULL|         6.99|      27.43| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       30.48|        NULL|     NULL|                NULL|\n|    NULL|02/05/2021 10:47:...| 02/05/2021 10:54:...|              NULL|      NULL|          76|          63|           NULL|         1.04|      15.43| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       18.48|        NULL|     NULL|                NULL|\n|       2|04/28/2021 04:21:...| 04/28/2021 05:09:...|                 N|         1|          33|          29|              1|        13.17|       43.5|  1.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        45.3|           1|        1|                 0.0|\n|    NULL|01/06/2021 04:45:...| 01/06/2021 05:29:...|              NULL|      NULL|          89|           1|           NULL|          0.0|       62.2|  1.0|    0.5|     10.58|       19.87|     NULL|                  0.3|       96.45|        NULL|     NULL|                NULL|\n|       2|01/09/2021 05:47:...| 01/09/2021 06:06:...|                 N|         1|          82|          56|              1|         2.01|       12.5|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        13.3|           2|        1|                 0.0|\n|    NULL|03/24/2021 12:46:...| 03/24/2021 01:05:...|              NULL|      NULL|          76|         122|           NULL|        10.07|      33.06| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       36.11|        NULL|     NULL|                NULL|\n|       2|02/25/2021 07:50:...| 02/25/2021 08:04:...|                 N|         1|          74|          43|              1|         1.37|        9.5|  0.0|    0.5|      2.06|         0.0|     NULL|                  0.3|       12.36|           1|        1|                 0.0|\n|       2|04/15/2021 05:44:...| 04/15/2021 06:10:...|                 N|         1|          17|          72|              1|         4.01|       18.5|  1.0|    0.5|      4.06|         0.0|     NULL|                  0.3|       24.36|           1|        1|                 0.0|\n|       2|02/24/2021 05:36:...| 02/24/2021 05:45:...|                 N|         1|          43|          75|              1|         1.13|        7.5|  1.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         9.3|           2|        1|                 0.0|\n|       2|04/25/2021 12:49:...| 04/25/2021 12:51:...|                 N|         1|          41|          41|              1|         0.58|        4.0|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         4.8|           2|        1|                 0.0|\n|       2|03/02/2021 05:59:...| 03/02/2021 06:43:...|                 N|         1|         197|         130|              1|         6.53|       30.0|  1.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        31.8|           1|        1|                 0.0|\n|       1|02/06/2021 02:29:...| 02/06/2021 02:41:...|                 N|         1|          74|         179|              1|          3.7|       13.0|  0.0|    0.5|      3.95|        6.12|     NULL|                  0.3|       23.87|           1|        1|                 0.0|\n|    NULL|03/27/2021 08:27:...| 03/27/2021 08:45:...|              NULL|      NULL|          97|         133|           NULL|         7.72|      24.63| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|       27.68|        NULL|     NULL|                NULL|\n|    NULL|02/17/2021 07:51:...| 02/17/2021 08:01:...|              NULL|      NULL|          61|          61|           NULL|         0.67|      11.95| 2.75|    0.0|       0.0|         0.0|     NULL|                  0.3|        15.0|        NULL|     NULL|                NULL|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "read_df.createOrReplaceTempView(\"read_table\")\n",
    "df=spark.sql(\"select  VendorID,sum(fare_amount) as total_fare from read_table group by VendorID \")\n",
    "spark.sql(\"select* from  read_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d527f1b5-fa1c-4603-bb7d-b04469d75727",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "read_df = read_df.withColumn(\"pickuptime\", to_timestamp(col(\"lpep_pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "taxitransformation = read_df.withWatermark(\"pickuptime\", \"10 minutes\") \\\n",
    "    .groupBy('trip_type', 'pickuptime').sum('fare_amount')\n",
    "writetsream = taxitransformation.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/Volumes/azuredatabricks_2000/default/writestream/\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/azuredatabricks_2000/default/checkpoint/\") \\\n",
    "    .start()\n",
    "writetsream.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataBricks-Realtime StreamingAnalysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
